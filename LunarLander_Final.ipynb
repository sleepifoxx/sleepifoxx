{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fde765",
   "metadata": {},
   "source": [
    "# 1. Định nghĩa bài toán\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cb8da",
   "metadata": {},
   "source": [
    "- Lunar Lander là một bài toán học tăng cường, cụ thể là 1 bài toán Quyết định Markov (MDP) rời rạc.\n",
    "$$M=\\{S,A,P,R,\\gamma\\}$$\n",
    "- Trong đó:\n",
    "##### Không gian trạng thái ($S$)\n",
    "- Theo định nghĩa từ Gymnasium, không gian trạng thái rời rạc $S \\subset R^8$ \n",
    "- Mỗi trạng thái $s\\in S$ là vector gồm 8 giá trị:\n",
    "$$s=[x,y,v_{x},v_{y},\\theta,\\omega,c_{l},c_{r}]$$\n",
    "- Với:\n",
    "\t- $x,y:$ toạ độ của con tàu theo trục $x,y$\n",
    "\t- $v_{x},v_{y}:$ vận tốc của con tàu theo các phương $x,y$\n",
    "\t- $\\theta:$ góc quay của con tàu\n",
    "\t- $\\omega:$ vận tốc quay của con tàu\n",
    "\t- $c_{l},c_{r}:$ biến nhị phân thể hiện sự tiếp đất của hai chân $left,right$\n",
    "##### Không gian hành động ($A$)\n",
    "- Theo định nghĩa từ Gymnasium, không gian hành động $A$ rời rạc gồm các hành động:\n",
    "\t- $a=0:$ không làm gì\n",
    "\t- $a=1:$ bật động cơ chính\n",
    "\t- $a=2:$ bật động cơ bên trái\n",
    "\t- $a=3:$ bật động cơ bên phải\n",
    "##### Xác suất chuyển trạng thái ($P$)\n",
    "##### Hàm phần thưởng ($R$)\n",
    "- Theo định nghĩa từ Gymnasium, phần thưởng nhận được sau khi chuyển từ trạng thái $s$ sang $s'$ là: $$R(s,s')=\\begin{cases}\n",
    "+10p \\text{ với mỗi chân tiếp xúc} \\\\\n",
    "-0.03p \\text{ với mỗi động cơ hai bên bật} \\\\\n",
    "-0.3p \\text{ với động cơ chính bật} \\\\\n",
    "-100p \\text{ với hạ cánh không thành công} \\\\\n",
    "+100p \\text{ với hạ cánh thành công} \\\\\n",
    "\\text{càng tăng/giảm khi tàu càng xa/gần mặt đất} \\\\\n",
    "\\text{càng tăng/giảm khi tốc độ tàu càng nhanh/chậm} \\\\\n",
    "\\text{càng giảm khi góc nghiêng tàu càng lớn}\n",
    "\\end{cases}$$\n",
    "##### Hệ số chiết khấu (discount factor: $\\gamma$)\n",
    "\n",
    "#### 2. Trạng thái kết thúc\n",
    "- Tàu hạ cánh thành công:\n",
    "\t- Cả 2 chân đều chạm đất: $c_{l}=c_{r}=1$\n",
    "\t- Vận tốc rơi nhỏ và góc nghiêng nhỏ\n",
    "\t- Tổng $\\text{reward} \\geq 200$\n",
    "- Tàu rơi hoặc lật:\n",
    "\t- Chạm đất với vận tốc lớn hoặc góc nghiêng lớn\n",
    "\t- Rơi ra khỏi màn hinh\n",
    "\t- Tổng $\\text{reward} \\leq -100$\n",
    "- Vượt quá số bước tối đa:\n",
    "\t- Mỗi episode có số step tối đa là 1000\n",
    "\t- Nếu không hạ cánh hay rơi trong khoảng số bước này, episode sẽ bị huỷ\n",
    "#### 3. Mục tiêu bài toán\n",
    "- Tìm chính sách tối ưu $\\pi^*:S\\to A$ sao cho kỳ vọng của tổng phần thưởng được tối đa\n",
    "$$\\pi^*=argmax_{\\pi}E_{\\pi}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t.R(s_{t},a_{t},s_{t+1}) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea51c5a",
   "metadata": {},
   "source": [
    "# 2. Cài đặt tổng quan\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07cfea",
   "metadata": {},
   "source": [
    "## 2.1. Thư viện, các hàm xử lí chung và môi trường LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf598718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install swig\n",
    "# !pip install gymnasium[box2d]\n",
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e7dbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from IPython.display import Video\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0d263b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, moving_average_window=50):\n",
    "    # Calculate moving average\n",
    "    moving_avg = np.convolve(rewards, np.ones(moving_average_window)/moving_average_window, mode='valid')\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(rewards, label='Rewards', alpha=0.5)\n",
    "    plt.plot(np.arange(moving_average_window - 1, len(rewards)), moving_avg, label='Moving Average', color='red')\n",
    "    plt.title('Rewards and Moving Average')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0369147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses, moving_average_window=50):\n",
    "    # Calculate moving average\n",
    "    moving_avg = np.convolve(losses, np.ones(moving_average_window)/moving_average_window, mode='valid')\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(losses, label='Losses', alpha=0.5)\n",
    "    plt.plot(np.arange(moving_average_window - 1, len(losses)), moving_avg, label='Moving Average', color='red')\n",
    "    plt.title('Losses and Moving Average')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "852bf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fuel_usage(fuel_usage, moving_average_window=50):\n",
    "    # Calculate moving average\n",
    "    moving_avg = np.convolve(fuel_usage, np.ones(moving_average_window)/moving_average_window, mode='valid')\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(fuel_usage, label='Fuel Usage', alpha=0.5)\n",
    "    plt.plot(np.arange(moving_average_window - 1, len(fuel_usage)), moving_avg, label='Moving Average', color='red')\n",
    "    plt.title('Fuel Usage and Moving Average')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Fuel Usage')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82330dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8938fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Dimension: 8, Action Dimension: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"State Dimension: {state_dim}, Action Dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0722c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_cost = {0: 0.0, 1: 0.3, 2: 0.03, 3: 0.03}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e162fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81e84f",
   "metadata": {},
   "source": [
    "## 2.2. Random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03aa47",
   "metadata": {},
   "source": [
    "- Trước hết, ta sẽ khởi tạo một lớp có tên `RandomTrain` để kiểm tra các hàm xử lý chung và đảm bảo rằng các chức năng cơ bản của môi trường hoạt động đúng như mong đợi. Việc kiểm tra này giúp phát hiện sớm các lỗi tiềm ẩn trước khi áp dụng các thuật toán huấn luyện phức tạp hơn. Đồng thời, quá trình này cũng cho phép ghi lại và hiển thị một video mẫu minh họa cách tác nhân hoạt động khi hành động được chọn một cách ngẫu nhiên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29b417b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTrain:\n",
    "    def __init__(self, env, episodes):\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.rewards = []\n",
    "        self.losses = []\n",
    "        self.fuel_usage = []\n",
    "        self.model = None\n",
    "\n",
    "    def train(self):\n",
    "        for episode in tqdm(range(self.episodes)):\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            total_fuel = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                total_fuel += fuel_cost[action]\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            self.rewards.append(total_reward)\n",
    "            self.fuel_usage.append(total_fuel)\n",
    "            #print(f\"Episode {episode + 1}/{self.episodes}, Reward: {total_reward}, Fuel Used: {total_fuel}\")\n",
    "            self.env.close()\n",
    "        return self.rewards, self.losses, self.fuel_usage\n",
    "    \n",
    "    def plot_results(self):\n",
    "        plot_rewards(self.rewards)\n",
    "        plot_fuel_usage(self.fuel_usage)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        return np.random.randint(0, action_dim-1)\n",
    "    \n",
    "    def display_sample_video(self, sample_episode=1):\n",
    "        video_render = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "        video_render = RecordVideo(video_render, \"videos\", episode_trigger=lambda x: True)\n",
    "        for _ in range(sample_episode):\n",
    "            state, _ = video_render.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.random.randint(1, 3)\n",
    "                state, reward, terminated, truncated, _ = video_render.step(action)\n",
    "                done = terminated or truncated\n",
    "            video_render.close()\n",
    "        video_files = glob.glob(\"videos/*.mp4\")\n",
    "        return Video(video_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dda23b",
   "metadata": {},
   "source": [
    "### Kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92bd333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_train = RandomTrain(env, episodes=500)\n",
    "# rewards, losses, fuel_usage = random_train.train()\n",
    "# random_train.plot_results()\n",
    "# random_train.display_sample_video(sample_episode=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a26a3",
   "metadata": {},
   "source": [
    "# 3. Q-Learning sử dụng Q-Table\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d3946",
   "metadata": {},
   "source": [
    "### a. Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be66cd",
   "metadata": {},
   "source": [
    "- Q-Learning là một thuật toán học tăng cường không mô hình sử dụng bảng Q-Table để lưu trữ và cập nhật giá trị kỳ vọng của mỗi hành động tại từng trạng thái.\n",
    "- Q-Table là một bảng trong đó mỗi phần tử `Q(s, a)` biểu thị giá trị kỳ vọng của việc thực hiện hành động `a` tại trạng thái `s` và sau đó tuân theo chính sách tối ưu.\n",
    "- Vì Q-learning chỉ áp dụng được cho không gian trạng thái `rời rạc`, nên cần lượng tử hóa (discretize) không gian trạng thái của LunarLander `(liên tục)` thành các bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d987d6",
   "metadata": {},
   "source": [
    "### b. Công thức toán học"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1621e",
   "metadata": {},
   "source": [
    "- Q-Table được cập nhật theo công thức Bellman:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $s$: trạng thái hiện tại (*current state*)\n",
    "- $a$: hành động đã chọn (*action taken*)\n",
    "- $r$: phần thưởng nhận được (*reward received*)\n",
    "- $s'$: trạng thái tiếp theo (*next state*)\n",
    "- $\\alpha \\in (0, 1)$: hệ số học (*learning rate*)\n",
    "- $\\gamma \\in (0, 1)$: hệ số chiết khấu (*discount factor*)\n",
    "- $\\max_{a'} Q(s', a')$: giá trị Q tối đa tại trạng thái tiếp theo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a96b8",
   "metadata": {},
   "source": [
    "### c. Triển khai code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c619993b",
   "metadata": {},
   "source": [
    "Các hàm chính xử lí thuật toán:\n",
    "\n",
    "`__init__`\n",
    "\n",
    "Hàm khởi tạo các tham số cần thiết để huấn luyện:\n",
    "- Các tham số của thuật toán.\n",
    "- **bins_size**: Số lượng đoạn chia mỗi chiều trong không gian trạng thái (discretization).\n",
    "- **state_bins**: Mỗi chiều của trạng thái được chia nhỏ thành các đoạn (bin).\n",
    "- **q_table**: Q-table 9 chiều chứa giá trị Q cho mỗi tổ hợp trạng thái và hành động. Kích thước:\n",
    "$$(\\text{bins\\_size})^6*2*2*\\text{số hành động}$$\n",
    "\n",
    "`get_state_bin(state)`\n",
    "\n",
    "- Lượng tử hóa (discretize) trạng thái liên tục thành các chỉ số bin rời rạc.\n",
    "- Dùng np.digitize để xác định chỉ số bin tương ứng.\n",
    "- Đảm bảo chỉ số nằm trong phạm vi hợp lệ.\n",
    "\n",
    "`get_action(state)`\n",
    "\n",
    "Triển khai chính sách ε-greedy:\n",
    "- Với xác suất ε, chọn hành động ngẫu nhiên (khám phá).\n",
    "- Ngược lại, chọn hành động có giá trị Q lớn nhất từ Q-table.\n",
    "\n",
    "`train()`\n",
    "\n",
    "Hàm huấn luyện tác tử trong nhiều vòng lặp (episode):\n",
    "\n",
    "1. Mỗi bước:\n",
    "   - Chọn hành động bằng `get_action(state)`\n",
    "   - Tương tác với môi trường, nhận phần thưởng $r$\n",
    "   - Xác định trạng thái kế tiếp $s'$\n",
    "   - Tính toán sai số TD:\n",
    "\n",
    "     $$\n",
    "     \\delta = \\left[ r + \\gamma \\cdot \\max_{a'} Q(s', a') \\right] - Q(s, a)\n",
    "     $$\n",
    "\n",
    "   - Cập nhật giá trị Q:\n",
    "\n",
    "     $$\n",
    "     Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\delta\n",
    "     $$\n",
    "\n",
    "2. Sau mỗi vòng lặp, giảm $\\varepsilon$ theo công thức:\n",
    "\n",
    "   $$\n",
    "   \\varepsilon \\leftarrow \\max(\\varepsilon_{\\text{min}}, \\varepsilon \\cdot \\varepsilon_{\\text{decay}})\n",
    "   $$\n",
    "\n",
    "3. Ghi nhận tổng phần thưởng và mức tiêu hao nhiên liệu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6474f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, env, episodes, alpha, gamma, epsilon, epsilon_decay, min_epsilon, bins_size):\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.bins_size = bins_size\n",
    "        self.rewards = []\n",
    "        self.fuel_usage = []\n",
    "\n",
    "        self.state_bins = [\n",
    "            np.linspace(-1.5, 1.5, bins_size),\n",
    "            np.linspace(-0.5, 1.5, bins_size),\n",
    "            np.linspace(-2, 2, bins_size),\n",
    "            np.linspace(-2, 2, bins_size),\n",
    "            np.linspace(-3.14, 3.14, bins_size),\n",
    "            np.linspace(-5, 5, bins_size),\n",
    "            np.array([0, 1]),\n",
    "            np.array([0, 1]),\n",
    "        ]\n",
    "\n",
    "        self.q_table = np.zeros((bins_size, bins_size, bins_size, bins_size,\n",
    "                                 bins_size, bins_size, 2, 2, action_dim))\n",
    "\n",
    "    def get_state_bin(self, state):\n",
    "        state_bins = []\n",
    "        for i in range(len(state)):\n",
    "            bin_index = np.digitize(state[i], self.state_bins[i]) - 1\n",
    "            # Clamp index within valid range\n",
    "            bin_index = min(max(bin_index, 0), len(self.state_bins[i]) - 1)\n",
    "            state_bins.append(bin_index)\n",
    "        return tuple(state_bins)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, action_dim-1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[self.get_state_bin(state)])\n",
    "\n",
    "    def train(self):\n",
    "        for episode in tqdm(range(self.episodes)):\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            total_fuel = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                total_fuel += fuel_cost[action]\n",
    "\n",
    "                # Update Q-table\n",
    "                state_bin = self.get_state_bin(state)\n",
    "                next_state_bin = self.get_state_bin(next_state)\n",
    "\n",
    "                best_next_action = np.argmax(self.q_table[next_state_bin])\n",
    "                td_target = reward + self.gamma * self.q_table[next_state_bin][best_next_action]\n",
    "                td_error = td_target - self.q_table[state_bin][action]\n",
    "\n",
    "                self.q_table[state_bin][action] += self.alpha * td_error\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "            self.rewards.append(total_reward)\n",
    "            self.fuel_usage.append(total_fuel)\n",
    "\n",
    "        return self.rewards, [], self.fuel_usage\n",
    "    \n",
    "    def plot_results(self):\n",
    "        plot_rewards(self.rewards)\n",
    "        plot_fuel_usage(self.fuel_usage)\n",
    "    \n",
    "    def display_sample_video(self, sample_episode=5):\n",
    "        video_render = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "        video_render = RecordVideo(video_render, \"videos\", episode_trigger=lambda x: True)\n",
    "        for _ in range(sample_episode):\n",
    "            state, _ = video_render.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                state, reward, terminated, truncated, _ = video_render.step(action)\n",
    "                done = terminated or truncated\n",
    "            video_render.close()\n",
    "        video_files = glob.glob(\"videos/*.mp4\")\n",
    "        return Video(video_files[-1])\n",
    "    \n",
    "    def test_model(self, episodes=500):\n",
    "        total_success = 0\n",
    "        testing = gym.make(\"LunarLander-v3\")\n",
    "        \n",
    "        for episode in tqdm(range(episodes)):\n",
    "            state, _ = testing.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            self.epsilon = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                state, reward, terminated, truncated, _ = testing.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "            if total_reward >= 200:\n",
    "                total_success += 1\n",
    "        print(f\"Success: {total_success}/{episodes} | Success Rate: {total_success / episodes * 100:.2f}%\")\n",
    "        testing.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da8949",
   "metadata": {},
   "source": [
    "### d. Kết quả chính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdc437aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = QTable(env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.996, min_epsilon=0.01, bins_size=8)\n",
    "# rewards, losses, fuel_usage = train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc601482",
   "metadata": {},
   "source": [
    "100%|██████████| 5000/5000 [02:31<00:00, 33.07it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c44087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f9724",
   "metadata": {},
   "source": [
    "![](./images/output(9).png)\n",
    "![](./images/output(10).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a85a450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.test_model(episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5db85e",
   "metadata": {},
   "source": [
    "100%|██████████| 500/500 [00:07<00:00, 62.67it/s]\n",
    "Success: 19/500 | Success Rate: 3.80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91dcd0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.display_sample_video(sample_episode=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac3b79",
   "metadata": {},
   "source": [
    "### e. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b27dcf",
   "metadata": {},
   "source": [
    "- Tỉ lệ thành công chỉ đạt **3.8%** sau 5000 episode và 500 tập kiểm tra cho thấy hiệu quả huấn luyện `Q-table` trên môi trường LunarLander là rất thấp.\n",
    "\n",
    "- Nguyên nhân có thể: \n",
    "1. Không gian trạng thái liên tục và có nhiều chiều\n",
    "\n",
    "    - Môi trường `LunarLander` có **8 chiều trạng thái liên tục**, khiến việc lượng tử hóa (discretization) trở nên khó khăn. Khi chia mỗi chiều thành `bins_size` đoạn, tổng số trạng thái rời rạc có thể lên tới:\n",
    "\n",
    "    $$\n",
    "    \\text{bins\\_size}^6 \\times 2 \\times 2\n",
    "    $$\n",
    "\n",
    "    - Ví dụ, nếu `bins_size = 10` thì số trạng thái là $10^6 \\times 2 \\times 2 = 4.000.000$ trạng thái, rất lớn cho bảng Q.\n",
    "\n",
    "2. Q-table tiêu tốn nhiều bộ nhớ và thời gian học  \n",
    "    - Khi để bins_size quá lớn sẽ gây ra hiện tượng tràn RAM\n",
    "    - Không thể khám phá đầy đủ tất cả các trạng thái trong khoảng thời gian huấn luyện hạn chế, dẫn đến việc **overfitting vào số ít trạng thái đã thấy**\n",
    "\n",
    "3. Không có khả năng tổng quát hóa\n",
    "   - Q-table chỉ lưu giá trị từng trạng thái cụ thể, không thể chia sẻ thông tin giữa các trạng thái gần nhau. Điều này khiến việc học trong không gian trạng thái liên tục kém hiệu quả.\n",
    "\n",
    "\n",
    "**$\\implies$ Giải quyết: Thay đổi thuật toán, sử dụng `Deep Q-Network` để xấp xỉ hàm Q với mạng neuron, khắc phục nhược điểm tiêu tốn quá nhiều bộ nhớ và không có khả năng tổng quát của `Q-Table`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d097a",
   "metadata": {},
   "source": [
    "# 4. Deep Q-Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e86c12",
   "metadata": {},
   "source": [
    "### a. Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a19c2",
   "metadata": {},
   "source": [
    "DQN là thuật toán tăng cường học sâu, nhằm giải quyết bài toán ra quyết định trong môi trường rời rạc, kết hợp:\n",
    "- Mạng Neuron học sâu\n",
    "- Q-Learning cổ điển\n",
    "\n",
    "Khác với Q-learning truyền thống chỉ sử dụng Q-Table để lưu trữ giá trị hành động, DQN sử dụng một mạng nơ-ron để xấp xỉ hàm Q, giúp mở rộng khả năng áp dụng sang các môi trường có không gian trạng thái lớn hoặc liên tục.\n",
    "\n",
    "Thuật toán DQN hoạt động bằng cách cho tác nhân tương tác với môi trường, thu thập kinh nghiệm dưới dạng các bộ dữ liệu (trạng thái, hành động, phần thưởng, trạng thái kế tiếp), và sử dụng các dữ liệu này để huấn luyện mạng nơ-ron sao cho đầu ra của nó gần đúng với giá trị Q tối ưu. Mục tiêu của DQN là tìm một hàm Q* sao cho tại mỗi trạng thái, hành động được chọn sẽ tối đa hóa tổng phần thưởng trong dài hạn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed317818",
   "metadata": {},
   "source": [
    "### b. Công thức toán học"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcb033",
   "metadata": {},
   "source": [
    "Tại mỗi bước huấn luyện, DQN tối thiểu hóa hàm mất mát sau:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim \\mathcal{D}} \\left[ \\left( y - Q(s, a; \\theta) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $ \\theta $ là tham số của mạng Q hiện tại.  \n",
    "- $ \\mathcal{D} $ là replay buffer (bộ nhớ kinh nghiệm) lưu trữ các trạng thái của môi trường.\n",
    "- $ y $ là giá trị mục tiêu, xác định bởi công thức: \n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\cdot \\max_{a'} Q(s', a'; \\theta^{-})\n",
    "$$\n",
    "\n",
    "- $ Q(s, a; \\theta) $: giá trị Q được ước lượng bởi mạng hiện tại.  \n",
    "- $ \\theta^{-} $: tham số của mạng Q mục tiêu (target network), được cập nhật định kỳ từ $ \\theta $.  \n",
    "- $ \\gamma \\in [0, 1] $: hệ số chiết khấu phần thưởng tương lai.\n",
    "\n",
    "\n",
    "Việc sử dụng `Replay Buffer` và `Target Q-Net` là hai cải tiến chính giúp DQN ổn định hơn và tránh hiện tượng học lệch, từ đó cho kết quả huấn luyện hiệu quả hơn so với các phương pháp Q-learning truyền thống.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d713db9",
   "metadata": {},
   "source": [
    "### c. Triển khai code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7b745",
   "metadata": {},
   "source": [
    "Lớp `DQNetwork` là một mạng nơ-ron sâu (deep neural network) được sử dụng để ước lượng hàm Q trong thuật toán Deep Q-Learning (DQN).\n",
    "\n",
    "Mạng này bao gồm:\n",
    "\n",
    "- Một lớp đầu vào với kích thước bằng số chiều trạng thái (`state_dim`).\n",
    "- Một lớp ẩn, mỗi lớp gồm 256 nút và sử dụng hàm kích hoạt ReLU.\n",
    "- Một lớp đầu ra với kích thước bằng số hành động (`action_dim`), trả về giá trị Q tương ứng với từng hành động."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "880e5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4b085",
   "metadata": {},
   "source": [
    "Lớp `DQNTrain` chịu trách nhiệm huấn luyện agent sử dụng thuật toán Deep Q-Network (DQN) trên môi trường như LunarLander-v3. Nó bao gồm các thành phần chính như sau:\n",
    "\n",
    "**Khởi tạo mô hình**\n",
    "- Khởi tạo mạng Q chính (`q_net`) và mạng Q mục tiêu (`target_net`).\n",
    "- Khởi tạo replay buffer để lưu trữ kinh nghiệm.\n",
    "- Chọn thuật toán tối ưu hóa (````adam````, RMSprop hoặc SGD).\n",
    "- Thiết lập các siêu tham số như epsilon, gamma, batch size, số tập huấn luyện,...\n",
    "\n",
    "**Chọn hành động (`select_action`)**\n",
    "- Dựa trên chính sách ε-greedy: chọn ngẫu nhiên hoặc hành động tốt nhất theo Q hiện tại. Chính sách này kết hợp giữa hành động ngẫu nhiên và hành động tối ưu:\n",
    "    - Với xác suất ε (epsilon), chọn hành động ngẫu nhiên.\n",
    "\n",
    "    - Với xác suất 1 - ε, chọn hành động tốt nhất (hành động có giá trị Q cao nhất).\n",
    "\n",
    "- Mục đích của ε-greedy là khuyến khích việc khám phá không gian trạng thái trong giai đoạn đầu và sau đó chuyển sang khai thác những hành động tối ưu đã học được khi epsilon giảm dần.\n",
    "\n",
    "**Huấn luyện từng bước (`train_step`)**\n",
    "- Lấy minibatch từ replay buffer.\n",
    "- Tính toán giá trị Q hiện tại và Q mục tiêu.\n",
    "- Tối ưu hóa bằng hàm mất mát MSE để cập nhật mạng Q.\n",
    "\n",
    "**Vòng lặp huấn luyện (`train`)**\n",
    "- Lặp qua các episode, cập nhật mạng mục tiêu định kỳ.\n",
    "- Theo dõi reward, loss và mức tiêu hao nhiên liệu qua từng tập.\n",
    "\n",
    "Các chú thích chi tiết ở trong code sau:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40fa70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrain:\n",
    "    def __init__(self, env, optimizer, epsilon, min_epsilon, decay, gamma, batch_size, episodes, target_update_freq, memory_size, learning_rate):\n",
    "        '''\n",
    "        - Khởi tạo các tham số cho DQN\n",
    "        - Khởi tạo optimizer (`adam`, RMSprop, SGD)\n",
    "        - Khởi tạo mạng Q và mạng Q mục tiêu\n",
    "        - Khởi tạo bộ nhớ (Replay Buffer) để lưu trữ các trạng thái của môi trường\n",
    "        - Khởi tạo các biến để theo dõi phần thưởng, tổn thất và mức tiêu thụ nhiên liệu\n",
    "        '''\n",
    "        self.env = env # Môi trường\n",
    "        self.episodes = episodes # Số lượng tập huấn luyện\n",
    "        self.epsilon = epsilon # Giá trị epsilon ban đầu\n",
    "        self.min_epsilon = min_epsilon # Giá trị epsilon tối thiểu\n",
    "        self.decay = decay # Hệ số giảm epsilon\n",
    "        self.gamma = gamma # Hệ số giảm giá cho phần thưởng\n",
    "        self.batch_size = batch_size # Kích thước batch cho việc huấn luyện\n",
    "        self.target_update_freq = target_update_freq # Tần suất cập nhật mạng Q mục tiêu\n",
    "        self.memory = deque(maxlen=memory_size) # Bộ nhớ để lưu trữ các trạng thái (replay buffer)\n",
    "        self.fuel_usage = [] # Danh sách để lưu trữ mức tiêu thụ nhiên liệu\n",
    "        self.rewards = [] # Danh sách để lưu trữ phần thưởng\n",
    "        self.losses = [] # Danh sách để lưu trữ loss\n",
    "        \n",
    "        self.q_net = DQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_net = DQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        if optimizer == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            self.optimizer = torch.optim.RMSprop(self.q_net.parameters(), lr=learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            self.optimizer = torch.optim.SGD(self.q_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        \n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        - Chọn hành động dựa trên epsilon-greedy\n",
    "        + Nếu giá trị ngẫu nhiên nhỏ hơn epsilon, chọn hành động ngẫu nhiên\n",
    "        + Ngược lại, chọn hành động có giá trị Q lớn nhất từ mạng Q\n",
    "        - Trả về chỉ số của hành động được chọn \n",
    "        '''\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                return self.q_net(state_tensor).argmax().item()\n",
    "            \n",
    "\n",
    "    def train_step(self):\n",
    "        ''''\n",
    "        - Huấn luyện mạng Q bằng cách lấy mẫu ngẫu nhiên từ bộ nhớ, nếu bộ nhớ lớn hơn kích thước batch\n",
    "        - Chuyển đổi các trạng thái, hành động, phần thưởng, trạng thái tiếp theo và done thành tensor\n",
    "        - Tính toán giá trị Q hiện tại và giá trị Q mục tiêu\n",
    "        - Tính toán loss bằng cách sử dụng hàm mất mát MSE\n",
    "        - Cập nhật trọng số của mạng Q bằng cách sử dụng thuật toán tối ưu hóa\n",
    "        - Trả về giá trị loss\n",
    "        '''\n",
    "        \n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            targets = rewards + self.gamma * max_next_q * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        - Huấn luyện DQN bằng cách lặp qua số lượng tập huấn luyện\n",
    "        - Trong mỗi tập, khởi tạo trạng thái và phần thưởng\n",
    "        - Lặp qua các bước trong mỗi tập cho đến khi hoàn thành\n",
    "        - Chọn hành động bằng cách sử dụng hàm select_action\n",
    "        - Thực hiện hành động và nhận trạng thái tiếp theo, phần thưởng và trạng thái hoàn thành\n",
    "        - Lưu trữ trạng thái vào bộ nhớ\n",
    "        - Tính toán loss bằng cách sử dụng hàm train_step\n",
    "        - Cập nhật mạng Q mục tiêu theo tần suất đã chỉ định\n",
    "        - Lưu trữ phần thưởng, loss và mức tiêu thụ nhiên liệu vào danh sách\n",
    "        - Trả về danh sách phần thưởng, loss và mức tiêu thụ nhiên liệu\n",
    "        '''\n",
    "        \n",
    "        pbar = tqdm(range(self.episodes), desc=\"Training DQN\")\n",
    "        \n",
    "        for ep in pbar:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "            total_fuel = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.memory.append((state, action, reward, next_state, float(done)))\n",
    "                \n",
    "                loss = self.train_step()\n",
    "                total_loss += loss\n",
    "                total_reward += reward\n",
    "                total_fuel += fuel_cost[action]\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "            if ep % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "                \n",
    "            if ep % 100 == 0:\n",
    "                average_reward = np.mean(self.rewards[-100:])\n",
    "                print(f\"Episode {ep}, Average Reward: {average_reward:.2f}\")\n",
    "                \n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
    "            self.rewards.append(total_reward)\n",
    "            self.losses.append(total_loss)\n",
    "            self.fuel_usage.append(total_fuel)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Reward\": total_reward,\n",
    "                \"Epsilon\": self.epsilon,\n",
    "                \"Loss\": total_loss,\n",
    "                \"Fuel Usage\": total_fuel\n",
    "            })\n",
    "\n",
    "        return self.rewards, self.losses, self.fuel_usage\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        '''\n",
    "        - Lưu trọng số của mạng Q vào tệp\n",
    "        '''\n",
    "        torch.save(self.target_net.state_dict(), path)\n",
    "\n",
    "\n",
    "    def load_model(self, path):\n",
    "        '''\n",
    "        - Tải trọng số của mạng Q từ tệp\n",
    "        - Đặt mạng Q mục tiêu ở chế độ đánh giá để không cập nhật trọng số trong quá trình kiểm tra\n",
    "        '''\n",
    "        self.target_net.load_state_dict(torch.load(path))\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        \n",
    "    def get_target_action(self, state):\n",
    "        ''' \n",
    "        - Chọn hành động từ mạng Q mục tiêu để kiểm tra\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return self.target_net(state_tensor).argmax().item()\n",
    "\n",
    "\n",
    "    def plot_results(self):\n",
    "        '''\n",
    "        - Vẽ biểu đồ phần thưởng, tổn thất và mức tiêu thụ nhiên liệu\n",
    "        - Sử dụng các hàm plot_rewards, plot_losses và plot_fuel_usage đã định nghĩa ở trên\n",
    "        '''\n",
    "        plot_rewards(self.rewards)\n",
    "        plot_losses(self.losses)\n",
    "        plot_fuel_usage(self.fuel_usage)\n",
    "        \n",
    "    \n",
    "    def test_model(self, episodes=500):\n",
    "        '''\n",
    "        - Kiểm tra mô hình đã được huấn luyện bằng cách chạy một số tập\n",
    "        - Trong mỗi tập, khởi tạo trạng thái và phần thưởng\n",
    "        - Lặp qua các bước trong mỗi tập cho đến khi hoàn thành\n",
    "        - Chọn hành động bằng cách sử dụng hàm get_target_action\n",
    "        - Thực hiện hành động và nhận trạng thái tiếp theo, phần thưởng và trạng thái hoàn thành\n",
    "        - Tính toán tổng phần thưởng\n",
    "        - Nếu tổng phần thưởng lớn hơn hoặc bằng 200, tăng biến thành công\n",
    "        - Trả về số lượng thành công và tỷ lệ thành công\n",
    "        '''\n",
    "        total_success = 0\n",
    "        testing = gym.make(\"LunarLander-v3\")\n",
    "        \n",
    "        for episode in tqdm(range(episodes)):\n",
    "            state, _ = testing.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.get_target_action(state)\n",
    "                state, reward, terminated, truncated, _ = testing.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "            if total_reward >= 200:\n",
    "                total_success += 1\n",
    "        print(f\"Success: {total_success}/{episodes} | Success Rate: {total_success / episodes * 100:.2f}%\")\n",
    "        testing.close()\n",
    "        \n",
    "        \n",
    "    def display_sample_video(self, sample_video=1):\n",
    "        '''\n",
    "        - Hiển thị video mẫu của mô hình đã được huấn luyện\n",
    "        - Sử dụng RecordVideo để ghi lại video trong môi trường\n",
    "        - Trong mỗi tập, khởi tạo trạng thái và phần thưởng\n",
    "        - Lặp qua các bước trong mỗi tập cho đến khi hoàn thành\n",
    "        - Chọn hành động bằng cách sử dụng hàm get_target_action\n",
    "        - Thực hiện hành động và nhận trạng thái tiếp theo, phần thưởng và trạng thái hoàn thành\n",
    "        - Đóng video sau khi hoàn thành\n",
    "        - Trả về video cuối cùng được ghi lại\n",
    "        '''\n",
    "        video_render = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "        video_render = RecordVideo(video_render, \"videos\", episode_trigger=lambda x: True)\n",
    "        \n",
    "        for _ in tqdm(range(sample_video)):\n",
    "            state, _ = video_render.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.get_target_action(state)\n",
    "                state, reward, terminated, truncated, _ = video_render.step(action)\n",
    "                done = terminated or truncated\n",
    "            video_render.close()\n",
    "        video_files = glob.glob(\"videos/*.mp4\")\n",
    "        return Video(video_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a6626",
   "metadata": {},
   "source": [
    "### d. Kết quả chính"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a8552",
   "metadata": {},
   "source": [
    "#### Kết quả với tối ưu RMSPROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eed56208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = DQNTrain(\n",
    "#     env=env,\n",
    "#     optimizer='rmsprop',\n",
    "#     epsilon=1.0,\n",
    "#     min_epsilon=0.01,\n",
    "#     decay=0.995,\n",
    "#     gamma=0.99,\n",
    "#     batch_size=128,\n",
    "#     episodes=2000,\n",
    "#     target_update_freq=10,\n",
    "#     memory_size=100000,\n",
    "#     learning_rate=1e-4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e284e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards, losses, fuel_usage = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d28a8",
   "metadata": {},
   "source": [
    "Training DQN:   0%|          | 0/2000 [00:00<?, ?it/s, Reward=-7.56, Epsilon=0.995, Loss=0, Fuel Usage=8.16]\n",
    "Episode 0, Average Reward: nan\n",
    "Training DQN:   5%|▌         | 101/2000 [00:28<11:40,  2.71it/s, Reward=-92.7, Epsilon=0.603, Loss=3.14e+3, Fuel Usage=8.31]\n",
    "Episode 100, Average Reward: -140.42\n",
    "Training DQN:  10%|█         | 201/2000 [02:01<1:11:47,  2.39s/it, Reward=-49.2, Epsilon=0.365, Loss=1.19e+4, Fuel Usage=82.6]\n",
    "Episode 200, Average Reward: -58.85\n",
    "Training DQN:  15%|█▌        | 301/2000 [06:08<1:15:53,  2.68s/it, Reward=-14.8, Epsilon=0.221, Loss=3.99e+3, Fuel Usage=62.1]   \n",
    "Episode 300, Average Reward: -22.66\n",
    "Training DQN:  20%|██        | 401/2000 [09:57<1:00:30,  2.27s/it, Reward=146, Epsilon=0.134, Loss=4.82e+3, Fuel Usage=49.7]   \n",
    "Episode 400, Average Reward: 33.97\n",
    "Training DQN:  25%|██▌       | 501/2000 [13:39<49:22,  1.98s/it, Reward=12.3, Epsilon=0.0812, Loss=2.98e+3, Fuel Usage=21.6]   \n",
    "Episode 500, Average Reward: 32.39\n",
    "Training DQN:  30%|███       | 601/2000 [16:53<52:56,  2.27s/it, Reward=-32.7, Epsilon=0.0492, Loss=8.74e+3, Fuel Usage=62.2]\n",
    "Episode 600, Average Reward: 104.68\n",
    "Training DQN:  35%|███▌      | 701/2000 [19:52<30:53,  1.43s/it, Reward=242, Epsilon=0.0298, Loss=4.9e+3, Fuel Usage=29.4]   \n",
    "Episode 700, Average Reward: 117.25\n",
    "Training DQN:  40%|████      | 801/2000 [22:26<19:48,  1.01it/s, Reward=238, Epsilon=0.018, Loss=2.47e+3, Fuel Usage=14.7]   \n",
    "Episode 800, Average Reward: 181.27\n",
    "Training DQN:  45%|████▌     | 901/2000 [24:09<14:24,  1.27it/s, Reward=254, Epsilon=0.0109, Loss=2.18e+3, Fuel Usage=12.2]\n",
    "Episode 900, Average Reward: 240.24\n",
    "Training DQN:  50%|█████     | 1001/2000 [25:51<16:08,  1.03it/s, Reward=257, Epsilon=0.01, Loss=1.98e+3, Fuel Usage=10.9] \n",
    "Episode 1000, Average Reward: 237.16\n",
    "Training DQN:  55%|█████▌    | 1101/2000 [27:23<12:54,  1.16it/s, Reward=258, Epsilon=0.01, Loss=1.14e+3, Fuel Usage=8.82]  \n",
    "Episode 1100, Average Reward: 244.40\n",
    "Training DQN:  60%|██████    | 1201/2000 [28:39<14:11,  1.07s/it, Reward=210, Epsilon=0.01, Loss=4.43e+3, Fuel Usage=88.9]\n",
    "Episode 1200, Average Reward: 260.15\n",
    "Training DQN:  65%|██████▌   | 1301/2000 [29:52<09:03,  1.29it/s, Reward=253, Epsilon=0.01, Loss=1.65e+3, Fuel Usage=12]  \n",
    "Episode 1300, Average Reward: 259.16\n",
    "Training DQN:  70%|███████   | 1401/2000 [31:02<06:07,  1.63it/s, Reward=257, Epsilon=0.01, Loss=1.65e+3, Fuel Usage=15.1]\n",
    "Episode 1400, Average Reward: 247.33\n",
    "Training DQN:  75%|███████▌  | 1501/2000 [32:15<06:23,  1.30it/s, Reward=278, Epsilon=0.01, Loss=1.6e+3, Fuel Usage=15.5]   \n",
    "Episode 1500, Average Reward: 245.86\n",
    "Training DQN:  80%|████████  | 1601/2000 [33:31<04:20,  1.53it/s, Reward=289, Epsilon=0.01, Loss=1.31e+3, Fuel Usage=15.6] \n",
    "Episode 1600, Average Reward: 256.02\n",
    "Training DQN:  85%|████████▌ | 1701/2000 [34:53<04:55,  1.01it/s, Reward=287, Epsilon=0.01, Loss=1.24e+3, Fuel Usage=17.5]\n",
    "Episode 1700, Average Reward: 252.03\n",
    "Training DQN:  90%|█████████ | 1801/2000 [36:07<01:56,  1.70it/s, Reward=247, Epsilon=0.01, Loss=732, Fuel Usage=7.65]    \n",
    "Episode 1800, Average Reward: 259.45\n",
    "Training DQN:  95%|█████████▌| 1901/2000 [37:18<00:57,  1.71it/s, Reward=291, Epsilon=0.01, Loss=994, Fuel Usage=15.5]    \n",
    "Episode 1900, Average Reward: 260.10\n",
    "Training DQN: 100%|██████████| 2000/2000 [38:25<00:00,  1.15s/it, Reward=237, Epsilon=0.01, Loss=1.84e+3, Fuel Usage=13.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40db8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91d2ae",
   "metadata": {},
   "source": [
    "![i1](images/output.png)\n",
    "![i2](images/output(1).png)\n",
    "![i3](images/output(2).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b57ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(\"dqn_lunarlander-2k-rmsprop.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c13d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.load_model(\"dqn_lunarlander-2k-rmsprop.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ddd63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.test_model(episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84e35c",
   "metadata": {},
   "source": [
    "Success: 414/500 | Success Rate: 82.80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cbf5797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.display_sample_video(sample_video=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5fde729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos-rmsprop/rl-video-episode-4.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"videos-rmsprop/rl-video-episode-4.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50912c56",
   "metadata": {},
   "source": [
    "#### Kết quả với tối ưu Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "883b377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = DQNTrain(\n",
    "#     env=env,\n",
    "#     optimizer='adam',\n",
    "#     epsilon=1.0,\n",
    "#     min_epsilon=0.01,\n",
    "#     decay=0.995,\n",
    "#     gamma=0.99,\n",
    "#     batch_size=128,\n",
    "#     episodes=2000,\n",
    "#     target_update_freq=10,\n",
    "#     memory_size=100000,\n",
    "#     learning_rate=1e-4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a4be0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewards, losses, fuel_usage = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d47cdc",
   "metadata": {},
   "source": [
    "Training DQN:   0%|          | 2/2000 [00:00<03:17, 10.13it/s, Reward=-161, Epsilon=0.99, Loss=3.35e+3, Fuel Usage=9.63]\n",
    "Episode 0, Average Reward: nan\n",
    "Training DQN:   5%|▌         | 101/2000 [00:29<14:36,  2.17it/s, Reward=-142, Epsilon=0.603, Loss=2.65e+3, Fuel Usage=11.7] \n",
    "Episode 100, Average Reward: -119.58\n",
    "Training DQN:  10%|█         | 201/2000 [02:08<1:06:03,  2.20s/it, Reward=-46.4, Epsilon=0.365, Loss=1e+4, Fuel Usage=68.4]   \n",
    "Episode 200, Average Reward: -68.83\n",
    "Training DQN:  15%|█▌        | 301/2000 [06:27<1:16:05,  2.69s/it, Reward=-14.3, Epsilon=0.221, Loss=3.74e+3, Fuel Usage=63.6]\n",
    "Episode 300, Average Reward: -16.85\n",
    "Training DQN:  20%|██        | 401/2000 [10:18<1:07:34,  2.54s/it, Reward=70.8, Epsilon=0.134, Loss=5.68e+3, Fuel Usage=48.4] \n",
    "Episode 400, Average Reward: 47.01\n",
    "Training DQN:  25%|██▌       | 501/2000 [13:52<33:58,  1.36s/it, Reward=249, Epsilon=0.0812, Loss=2.61e+3, Fuel Usage=9.39]    \n",
    "Episode 500, Average Reward: 74.07\n",
    "Training DQN:  30%|███       | 601/2000 [16:42<31:56,  1.37s/it, Reward=234, Epsilon=0.0492, Loss=3.54e+3, Fuel Usage=7.86]   \n",
    "Episode 600, Average Reward: 152.28\n",
    "Training DQN:  35%|███▌      | 701/2000 [19:09<19:15,  1.12it/s, Reward=279, Epsilon=0.0298, Loss=2.73e+3, Fuel Usage=11.9]  \n",
    "Episode 700, Average Reward: 164.76\n",
    "Training DQN:  40%|████      | 801/2000 [21:01<19:31,  1.02it/s, Reward=279, Epsilon=0.018, Loss=3.23e+3, Fuel Usage=9.9]    \n",
    "Episode 800, Average Reward: 221.37\n",
    "Training DQN:  45%|████▌     | 901/2000 [22:35<15:02,  1.22it/s, Reward=296, Epsilon=0.0109, Loss=2.52e+3, Fuel Usage=11.7]  \n",
    "Episode 900, Average Reward: 233.87\n",
    "Training DQN:  50%|█████     | 1001/2000 [23:55<13:15,  1.26it/s, Reward=283, Epsilon=0.01, Loss=2.01e+3, Fuel Usage=12.6] \n",
    "Episode 1000, Average Reward: 260.68\n",
    "Training DQN:  55%|█████▌    | 1101/2000 [25:12<11:50,  1.27it/s, Reward=234, Epsilon=0.01, Loss=1.44e+3, Fuel Usage=20.4]  \n",
    "Episode 1100, Average Reward: 259.33\n",
    "Training DQN:  60%|██████    | 1201/2000 [26:31<08:43,  1.53it/s, Reward=266, Epsilon=0.01, Loss=1.78e+3, Fuel Usage=9.48]  \n",
    "Episode 1200, Average Reward: 255.35\n",
    "Training DQN:  65%|██████▌   | 1301/2000 [27:37<06:49,  1.71it/s, Reward=287, Epsilon=0.01, Loss=1.04e+3, Fuel Usage=10.5]\n",
    "Episode 1300, Average Reward: 266.93\n",
    "Training DQN:  70%|███████   | 1401/2000 [28:38<05:51,  1.71it/s, Reward=245, Epsilon=0.01, Loss=1.12e+3, Fuel Usage=9.36]\n",
    "Episode 1400, Average Reward: 273.05\n",
    "Training DQN:  75%|███████▌  | 1501/2000 [29:40<04:19,  1.92it/s, Reward=276, Epsilon=0.01, Loss=813, Fuel Usage=7.26]    \n",
    "Episode 1500, Average Reward: 278.39\n",
    "Training DQN:  80%|████████  | 1601/2000 [30:41<03:21,  1.98it/s, Reward=270, Epsilon=0.01, Loss=982, Fuel Usage=4.11]    \n",
    "Episode 1600, Average Reward: 277.69\n",
    "Training DQN:  85%|████████▌ | 1701/2000 [31:43<02:36,  1.91it/s, Reward=302, Epsilon=0.01, Loss=1.78e+3, Fuel Usage=10.3]\n",
    "Episode 1700, Average Reward: 267.48\n",
    "Training DQN:  90%|█████████ | 1801/2000 [32:41<01:48,  1.84it/s, Reward=308, Epsilon=0.01, Loss=1.09e+3, Fuel Usage=9.18]\n",
    "Episode 1800, Average Reward: 255.86\n",
    "Training DQN:  95%|█████████▌| 1901/2000 [33:36<00:48,  2.05it/s, Reward=252, Epsilon=0.01, Loss=854, Fuel Usage=6.69]     \n",
    "Episode 1900, Average Reward: 248.64\n",
    "Training DQN: 100%|██████████| 2000/2000 [34:30<00:00,  1.04s/it, Reward=276, Epsilon=0.01, Loss=1.87e+3, Fuel Usage=14.4]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a18d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbeef95",
   "metadata": {},
   "source": [
    "![](images/output(3).png)\n",
    "![](images/output(4).png)\n",
    "![](images/output(5).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2934af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(\"dqn_lunarlander-2k-adam.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43bb12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.load_model(\"dqn_lunarlander-2k-adam.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "baa348ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.test_model(episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f327f95",
   "metadata": {},
   "source": [
    "Success: 484/500 | Success Rate: 96.80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dbe99dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.display_sample_video(sample_video=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff551ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos-adam/rl-video-episode-4.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"videos-adam/rl-video-episode-4.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1887cd",
   "metadata": {},
   "source": [
    "#### **So sánh**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfb2eb",
   "metadata": {},
   "source": [
    "Với cùng thông số, `adam` cho kết quả tốt hơn so với `rmsprop`\n",
    "- Average Reward của `adam` đạt trên 200 ep700-800 còn `rmsprop` là ep800-900\n",
    "- Độ chính xác khi hạ cánh của `adam` rất cao khi đạt tỉ lệ 96.80% trong khi `rmsprop` là 82.80%\n",
    "- Thời gian chạy cũng nhanh hơn khi `adam` chỉ mất 34p còn `rmsprop` mất 38p\n",
    "\n",
    "$\\implies$ Lý do:\n",
    "- `adam` tích hợp cả momentum và `rmsprop` nên hiệu quả học thường cao hơn đối với môi trường phức tạp\n",
    "- Môi trường ổn định (không gió, không nhiễu) nên cần một thuật toán có thể khiến loss mô hình hội tụ nhanh như `adam`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938bc7f",
   "metadata": {},
   "source": [
    "#### Adam với learning rate khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0910590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = DQNTrain(\n",
    "#     env=env,\n",
    "#     optimizer='adam',\n",
    "#     epsilon=1.0,\n",
    "#     min_epsilon=0.01,\n",
    "#     decay=0.995,\n",
    "#     gamma=0.99,\n",
    "#     batch_size=128,\n",
    "#     episodes=2000,\n",
    "#     target_update_freq=10,\n",
    "#     memory_size=100000,\n",
    "#     learning_rate=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ddf380e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards, losses, fuel_usage = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6590d7e",
   "metadata": {},
   "source": [
    "Training DQN:   0%|          | 0/2000 [00:00<?, ?it/s, Reward=-88.5, Epsilon=0.995, Loss=0, Fuel Usage=7.5]\n",
    "Episode 0, Average Reward: nan\n",
    "Training DQN:   5%|▌         | 101/2000 [00:28<10:40,  2.97it/s, Reward=-75.1, Epsilon=0.603, Loss=2.69e+3, Fuel Usage=7.08]\n",
    "Episode 100, Average Reward: -136.95\n",
    "Training DQN:  10%|█         | 201/2000 [01:43<31:42,  1.06s/it, Reward=-154, Epsilon=0.365, Loss=1.29e+4, Fuel Usage=51.2] \n",
    "Episode 200, Average Reward: -80.68\n",
    "Training DQN:  15%|█▌        | 301/2000 [05:02<44:12,  1.56s/it, Reward=-116, Epsilon=0.221, Loss=6.74e+3, Fuel Usage=26.3]   \n",
    "Episode 300, Average Reward: -110.11\n",
    "Training DQN:  20%|██        | 401/2000 [08:44<36:53,  1.38s/it, Reward=-40.3, Epsilon=0.134, Loss=1.28e+3, Fuel Usage=11.8]  \n",
    "Episode 400, Average Reward: -68.13\n",
    "Training DQN:  25%|██▌       | 501/2000 [11:03<18:17,  1.37it/s, Reward=248, Epsilon=0.0812, Loss=4.29e+3, Fuel Usage=15.4]   \n",
    "Episode 500, Average Reward: 19.42\n",
    "Training DQN:  30%|███       | 602/2000 [11:41<05:16,  4.41it/s, Reward=-215, Epsilon=0.0489, Loss=1.11e+3, Fuel Usage=4.35] \n",
    "Episode 600, Average Reward: -150.49\n",
    "Training DQN:  35%|███▌      | 701/2000 [12:08<05:46,  3.75it/s, Reward=-208, Epsilon=0.0298, Loss=2.45e+3, Fuel Usage=8.43] \n",
    "Episode 700, Average Reward: -192.99\n",
    "Training DQN:  40%|████      | 801/2000 [12:41<08:00,  2.50it/s, Reward=48.7, Epsilon=0.018, Loss=4.36e+3, Fuel Usage=4.56]  \n",
    "Episode 800, Average Reward: -55.63\n",
    "Training DQN:  45%|████▌     | 901/2000 [14:05<21:35,  1.18s/it, Reward=186, Epsilon=0.0109, Loss=3.02e+4, Fuel Usage=59.9]  \n",
    "Episode 900, Average Reward: -1.02\n",
    "Training DQN:  50%|█████     | 1001/2000 [15:49<19:03,  1.14s/it, Reward=236, Epsilon=0.01, Loss=1.38e+4, Fuel Usage=11.1]   \n",
    "Episode 1000, Average Reward: 30.97\n",
    "Training DQN:  55%|█████▌    | 1101/2000 [17:38<15:28,  1.03s/it, Reward=-124, Epsilon=0.01, Loss=6.45e+3, Fuel Usage=6.96] \n",
    "Episode 1100, Average Reward: 48.55\n",
    "Training DQN:  60%|██████    | 1201/2000 [19:21<11:32,  1.15it/s, Reward=272, Epsilon=0.01, Loss=1.08e+4, Fuel Usage=23.3]  \n",
    "Episode 1200, Average Reward: -50.40\n",
    "Training DQN:  65%|██████▌   | 1301/2000 [21:04<12:22,  1.06s/it, Reward=271, Epsilon=0.01, Loss=9.94e+3, Fuel Usage=24.7]  \n",
    "Episode 1300, Average Reward: 14.16\n",
    "Training DQN:  70%|███████   | 1401/2000 [22:50<10:03,  1.01s/it, Reward=302, Epsilon=0.01, Loss=1.15e+4, Fuel Usage=13.2]  \n",
    "Episode 1400, Average Reward: 115.29\n",
    "Training DQN:  75%|███████▌  | 1501/2000 [24:50<08:00,  1.04it/s, Reward=-102, Epsilon=0.01, Loss=3.91e+3, Fuel Usage=16.3] \n",
    "Episode 1500, Average Reward: 134.05\n",
    "Training DQN:  80%|████████  | 1601/2000 [26:40<05:56,  1.12it/s, Reward=263, Epsilon=0.01, Loss=4.96e+3, Fuel Usage=8.25]     \n",
    "Episode 1600, Average Reward: 93.36\n",
    "Training DQN:  85%|████████▌ | 1701/2000 [28:14<03:33,  1.40it/s, Reward=-8.37, Epsilon=0.01, Loss=7.22e+3, Fuel Usage=13.9]\n",
    "Episode 1700, Average Reward: -6.29\n",
    "Training DQN:  90%|█████████ | 1801/2000 [29:39<02:46,  1.19it/s, Reward=194, Epsilon=0.01, Loss=1.23e+4, Fuel Usage=12.9]     \n",
    "Episode 1800, Average Reward: -190.80\n",
    "Training DQN:  95%|█████████▌| 1901/2000 [30:54<01:09,  1.42it/s, Reward=-251, Epsilon=0.01, Loss=6.05e+3, Fuel Usage=1.98]    \n",
    "Episode 1900, Average Reward: -71.62\n",
    "Training DQN: 100%|██████████| 2000/2000 [32:28<00:00,  1.03it/s, Reward=264, Epsilon=0.01, Loss=1.76e+4, Fuel Usage=12.2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ae5ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466e246",
   "metadata": {},
   "source": [
    "![](./images/output(6).png)\n",
    "![](./images/output(7).png)\n",
    "![](./images/output(8).png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15b0582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"dqn_lunarlander-2k-adam-1e-3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a9e882bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.load_model(\"dqn_lunarlander-2k-adam-1e-3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3fc328ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test_model(episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7021a3",
   "metadata": {},
   "source": [
    "Success: 187/500 | Success Rate: 37.40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7aa1cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.display_sample_video(sample_video=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7eb4c296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos-adam-1e-3/rl-video-episode-4.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"videos-adam-1e-3/rl-video-episode-4.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1270b4",
   "metadata": {},
   "source": [
    "#### **So sánh**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27639200",
   "metadata": {},
   "source": [
    "Với cùng thuật toán tối ưu `adam`, thay đổi `learning_rate` từ (1e-4) sang (1e-3).\n",
    "Kết quả cho thấy: \n",
    "- Success rate của 1e-3 đạt 37.2%, hiệu suất giảm rõ rệt so với 96.8% (1e-4).\n",
    "- Thời gian training (kì vọng giảm rõ rệt) nhưng mất đến 32p, gần tương đương với 34p của (1e-4) mà hiệu suất giảm rõ rệt.\n",
    "- Điểm thưởng không ổn định như (1e-4).\n",
    "- Loss cũng cho thấy sự không ổn định của (1e-3) (có thể là do vấn đề liên quan đến exploding-gradient khi loss tăng cao).\n",
    "\n",
    "**$\\implies$ Việc giảm `learning_rate` xuống thấp có thể khiến hiệu suất mô hình giảm trong khi thời gian training mô hình không thay đổi đáng kể**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2ec08",
   "metadata": {},
   "source": [
    "### e. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121eb22",
   "metadata": {},
   "source": [
    "- Trong môi trường rời rạc tốt (không gió, không nhiễu), thuật toán DQN hoạt động rất hiệu quả có thể đạt hiệu suất trên **99%** nếu training episodes đủ nhiều, có thể sử dụng `lunarlander-5k.pth` để chứng minh hiệu suất.\n",
    "- Lý do tại sao DQN lại hoạt động hiệu suất cao như vậy:\n",
    "    - Không gian hành động nhỏ: Với chỉ 4 hành động rời rạc (không làm gì, bắn động cơ chính, bắn động cơ bên trái, bắn động cơ bên phải), DQN có thể dễ dàng học và cập nhật hàm Q cho từng hành động.\n",
    "    - Tập trạng thái có thể được biểu diễn tốt qua mạng neuron: Trạng thái gồm 8 giá trị thực (tọa độ, vận tốc, góc, trạng thái tiếp xúc), giúp mạng học các đặc trưng quan trọng mà không cần xử lý dữ liệu phức tạp như hình ảnh.\n",
    "    - Phản hồi phần thưởng rõ ràng, có cấu trúc: Phần thưởng trong LunarLander được thiết kế khuyến khích đáp xuống nhẹ nhàng và phạt cho các hành vi nguy hiểm, giúp DQN dễ học thông qua phản hồi.\n",
    "    - Không có yếu tố ngẫu nhiên mạnh (như gió, nhiễu loạn): Việc môi trường có tính ổn định tương đối cao giúp mạng học dễ dàng và ổn định hơn.\n",
    "    - Sử dụng các kĩ thuật `Replay Buffer` và `Target Q-Network` giúp mô hình học ổn định hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6ad48fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chứng minh hiệu suất của mô hình đã được huấn luyện 5k episodes\n",
    "# test = DQNTrain(\n",
    "#     env=env,\n",
    "#     optimizer='adam',\n",
    "#     epsilon=1.0,\n",
    "#     min_epsilon=0.01,\n",
    "#     decay=0.995,\n",
    "#     gamma=0.99,\n",
    "#     batch_size=128,\n",
    "#     episodes=2000,\n",
    "#     target_update_freq=10,\n",
    "#     memory_size=100000,\n",
    "#     learning_rate=1e-4\n",
    "# )\n",
    "\n",
    "# test.load_model(\"lunarlander-5k.pth\")\n",
    "# test.test_model(episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b6b60",
   "metadata": {},
   "source": [
    "# 5. Double Deep Q-Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1324f56",
   "metadata": {},
   "source": [
    "Với mô hình LunarLander truyền thống, khi không có sự tác động mạnh của môi trường như gió hoặc nhiễu loạn gió, đặc trưng học nhanh của mô hình (đặc biệt khi sử dụng thuật toán tối ưu `Adam`) giúp `DQN` đạt hiệu suất rất cao. Điều này có thể là do:\n",
    "- **Không gian trạng thái ổn định và dễ dự đoán**: Khi loại bỏ các yếu tố gây nhiễu như gió, môi trường trở nên nhất quán hơn giữa các lần huấn luyện. Điều này giúp mạng neuron trong DQN dễ dàng học được các quy luật điều khiển phù hợp, mà không cần phải xử lý các tình huống ngoại lệ hay biến động bất thường."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4eb0e",
   "metadata": {},
   "source": [
    "## a. Môi trường gió"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddb9a7",
   "metadata": {},
   "source": [
    "Chúng ta sẽ test thử mô hình DQN với môi trường nhiễu động do gió:\n",
    "- wind_power=15 (tốc độ gió)\n",
    "- turbulence_power=1.5 (nhiễu loạn gió)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bcb5be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_env = gym.make(\"LunarLander-v3\",gravity=-10.0, enable_wind=True, wind_power=15.0, turbulence_power=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef8828",
   "metadata": {},
   "source": [
    "## b. DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a004ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_env_dqn_train = DQNTrain(\n",
    "    env=win_env,\n",
    "    optimizer='adam',\n",
    "    epsilon=1.0,\n",
    "    min_epsilon=0.01,\n",
    "    decay=0.995,\n",
    "    gamma=0.99,\n",
    "    batch_size=128,\n",
    "    episodes=2000,\n",
    "    target_update_freq=10,\n",
    "    memory_size=100000,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9393a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward, losses, fuel_usage = win_env_dqn_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e9051",
   "metadata": {},
   "source": [
    "Training DQN:   0%|          | 2/2000 [00:00<03:24,  9.76it/s, Reward=-121, Epsilon=0.99, Loss=5.15e+3, Fuel Usage=10.6]\n",
    "Episode 0, Average Reward: nan\n",
    "Training DQN:   5%|▌         | 101/2000 [00:25<09:03,  3.50it/s, Reward=-209, Epsilon=0.603, Loss=2.18e+3, Fuel Usage=12.4] \n",
    "Episode 100, Average Reward: -238.64\n",
    "Training DQN:  10%|█         | 201/2000 [01:07<11:44,  2.56it/s, Reward=-58.4, Epsilon=0.365, Loss=1.59e+3, Fuel Usage=8.85] \n",
    "Episode 200, Average Reward: -146.54\n",
    "Training DQN:  15%|█▌        | 301/2000 [02:40<1:05:00,  2.30s/it, Reward=-79.9, Epsilon=0.221, Loss=1.43e+4, Fuel Usage=89.9]\n",
    "Episode 300, Average Reward: -97.41\n",
    "Training DQN:  20%|██        | 401/2000 [04:48<23:30,  1.13it/s, Reward=-84, Epsilon=0.134, Loss=2.94e+3, Fuel Usage=14.9]    \n",
    "Episode 400, Average Reward: -5.58\n",
    "Training DQN:  25%|██▌       | 501/2000 [06:53<43:47,  1.75s/it, Reward=125, Epsilon=0.0812, Loss=1.43e+4, Fuel Usage=75.4]  \n",
    "Episode 500, Average Reward: 17.24\n",
    "Training DQN:  30%|███       | 601/2000 [08:55<21:45,  1.07it/s, Reward=-42.4, Epsilon=0.0492, Loss=7.16e+3, Fuel Usage=18]  \n",
    "Episode 600, Average Reward: 47.06\n",
    "Training DQN:  35%|███▌      | 701/2000 [11:24<36:02,  1.66s/it, Reward=222, Epsilon=0.0298, Loss=9.36e+3, Fuel Usage=45.6]  \n",
    "Episode 700, Average Reward: 107.95\n",
    "Training DQN:  40%|████      | 801/2000 [14:18<35:43,  1.79s/it, Reward=-123, Epsilon=0.018, Loss=1.36e+4, Fuel Usage=75.9]  \n",
    "Episode 800, Average Reward: 115.25\n",
    "Training DQN:  45%|████▌     | 901/2000 [17:11<35:31,  1.94s/it, Reward=-284, Epsilon=0.0109, Loss=8.77e+3, Fuel Usage=61.9] \n",
    "Episode 900, Average Reward: 2.45\n",
    "Training DQN:  50%|█████     | 1001/2000 [20:12<30:24,  1.83s/it, Reward=-93.5, Epsilon=0.01, Loss=6.2e+3, Fuel Usage=61]    \n",
    "Episode 1000, Average Reward: -6.84\n",
    "Training DQN:  55%|█████▌    | 1101/2000 [22:58<20:55,  1.40s/it, Reward=209, Epsilon=0.01, Loss=2.4e+3, Fuel Usage=46.7]   \n",
    "Episode 1100, Average Reward: 53.16\n",
    "Training DQN:  60%|██████    | 1201/2000 [25:49<23:02,  1.73s/it, Reward=153, Epsilon=0.01, Loss=5.66e+3, Fuel Usage=59]    \n",
    "Episode 1200, Average Reward: 54.31\n",
    "Training DQN:  65%|██████▌   | 1301/2000 [28:06<14:09,  1.22s/it, Reward=274, Epsilon=0.01, Loss=2.31e+3, Fuel Usage=13.5]  \n",
    "Episode 1300, Average Reward: 111.35\n",
    "Training DQN:  70%|███████   | 1401/2000 [30:11<09:53,  1.01it/s, Reward=248, Epsilon=0.01, Loss=2.68e+3, Fuel Usage=16.1]  \n",
    "Episode 1400, Average Reward: 156.67\n",
    "Training DQN:  75%|███████▌  | 1501/2000 [32:06<09:58,  1.20s/it, Reward=306, Epsilon=0.01, Loss=2.57e+3, Fuel Usage=24.9]  \n",
    "Episode 1500, Average Reward: 162.58\n",
    "Training DQN:  80%|████████  | 1601/2000 [33:45<05:53,  1.13it/s, Reward=238, Epsilon=0.01, Loss=2.21e+3, Fuel Usage=32.5]  \n",
    "Episode 1600, Average Reward: 198.48\n",
    "Training DQN:  85%|████████▌ | 1701/2000 [35:30<03:22,  1.47it/s, Reward=246, Epsilon=0.01, Loss=2.62e+3, Fuel Usage=7.68]  \n",
    "Episode 1700, Average Reward: 201.26\n",
    "Training DQN:  90%|█████████ | 1801/2000 [36:59<02:58,  1.12it/s, Reward=254, Epsilon=0.01, Loss=4.39e+3, Fuel Usage=23.9]  \n",
    "Episode 1800, Average Reward: 225.50\n",
    "Training DQN:  95%|█████████▌| 1901/2000 [38:22<01:34,  1.05it/s, Reward=249, Epsilon=0.01, Loss=3.91e+3, Fuel Usage=8.25]  \n",
    "Episode 1900, Average Reward: 212.99\n",
    "Training DQN: 100%|██████████| 2000/2000 [40:06<00:00,  1.20s/it, Reward=249, Epsilon=0.01, Loss=2.73e+3, Fuel Usage=9.39]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7ad9d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win_env_dqn_train.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2aeb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win_env_dqn_train.save_model(\"dqn_lunarlander-win-env-2k-adam.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b46d3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win_env_dqn_train.test_model(episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e970a",
   "metadata": {},
   "source": [
    "100%|██████████| 500/500 [00:31<00:00, 15.65it/s]\n",
    "Success: 417/500 | Success Rate: 83.40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "77e9a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win_env_dqn_train.display_sample_video(sample_video=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2eb1a5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos-dqn-wind/rl-video-episode-4.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"videos-dqn-wind/rl-video-episode-4.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc727b8",
   "metadata": {},
   "source": [
    "**Nhận xét:**\n",
    "Có thể thấy rằng khi hoạt động trong môi trường có sự nhiễu loạn (như gió và biến động môi trường), hiệu suất của DQN giảm rõ rệt. Cụ thể, mức average reward > 200 chỉ đạt được sau khoảng 1700 episodes, so với mốc 800 episodes trong môi trường tiêu chuẩn. Tỷ lệ thành công (success rate) cũng giảm, chỉ còn 83%, thay vì >90% như trước. Đồng thời, thời gian huấn luyện tăng đáng kể lên đến khoảng 40 phút — cho thấy mức độ khó khăn tăng lên khi môi trường trở nên bất định hơn.\n",
    "Điều này có thể là do:\n",
    "- DQN dễ overfit vào các mẫu trạng thái quen thuộc: DQN có xu hướng học \"quá mức\" vào các mẫu quen thuộc, không có khả năng thay đổi để ứng phó với môi trường mới."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f14e3",
   "metadata": {},
   "source": [
    "## c. Tối ưu với DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cedf73",
   "metadata": {},
   "source": [
    "Nhận thấy sự thiếu sót của DQN trong môi trường có nhiễu loạn, dưới đây là triển khai của mô hình Double DQN (DDQN) cho hiệu suất tốt hơn ở môi trường nhiễu loạn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73ae20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ee603de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrain:\n",
    "    def __init__(self, env, optimizer, epsilon, min_epsilon, decay, gamma, batch_size, episodes, target_update_freq, memory_size, learning_rate):\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay = decay\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.fuel_usage = []\n",
    "        self.rewards = []\n",
    "        self.losses = []\n",
    "\n",
    "        self.q_net = DQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_net = DQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            self.optimizer = torch.optim.RMSprop(self.q_net.parameters(), lr=learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            self.optimizer = torch.optim.SGD(self.q_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                return self.q_net(state_tensor).argmax().item()\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_net(next_states).argmax(1).unsqueeze(1)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        pbar = tqdm(range(self.episodes), desc=\"Training DDQN\")\n",
    "        for ep in pbar:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "            total_fuel = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.memory.append((state, action, reward, next_state, float(done)))\n",
    "\n",
    "                loss = self.train_step()\n",
    "                total_loss += loss\n",
    "                total_reward += reward\n",
    "                total_fuel += fuel_cost[action]\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            if ep % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "            if ep % 100 == 0:\n",
    "                average_reward = np.mean(self.rewards[-100:])\n",
    "                print(f\"Episode {ep}, Average Reward: {average_reward:.2f}\")\n",
    "\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
    "            self.rewards.append(total_reward)\n",
    "            self.losses.append(total_loss)\n",
    "            self.fuel_usage.append(total_fuel)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Reward\": total_reward,\n",
    "                \"Epsilon\": self.epsilon,\n",
    "                \"Loss\": total_loss,\n",
    "                \"Fuel Usage\": total_fuel\n",
    "            })\n",
    "\n",
    "        return self.rewards, self.losses, self.fuel_usage\n",
    "\n",
    "    def save_model(self, path):\n",
    "        '''\n",
    "        - Lưu trọng số của mạng Q vào tệp\n",
    "        '''\n",
    "        torch.save(self.target_net.state_dict(), path)\n",
    "\n",
    "\n",
    "    def load_model(self, path):\n",
    "        '''\n",
    "        - Tải trọng số của mạng Q từ tệp\n",
    "        - Đặt mạng Q mục tiêu ở chế độ đánh giá để không cập nhật trọng số trong quá trình kiểm tra\n",
    "        '''\n",
    "        self.target_net.load_state_dict(torch.load(path))\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        \n",
    "    def get_target_action(self, state):\n",
    "        ''' \n",
    "        - Chọn hành động từ mạng Q mục tiêu để kiểm tra\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return self.target_net(state_tensor).argmax().item()\n",
    "\n",
    "\n",
    "    def plot_results(self):\n",
    "        '''\n",
    "        - Vẽ biểu đồ phần thưởng, tổn thất và mức tiêu thụ nhiên liệu\n",
    "        - Sử dụng các hàm plot_rewards, plot_losses và plot_fuel_usage đã định nghĩa ở trên\n",
    "        '''\n",
    "        plot_rewards(self.rewards)\n",
    "        plot_losses(self.losses)\n",
    "        plot_fuel_usage(self.fuel_usage)\n",
    "        \n",
    "    \n",
    "    def test_model(self, episodes=500):\n",
    "        '''\n",
    "        - Kiểm tra mô hình đã được huấn luyện bằng cách chạy một số tập\n",
    "        - Trong mỗi tập, khởi tạo trạng thái và phần thưởng\n",
    "        - Lặp qua các bước trong mỗi tập cho đến khi hoàn thành\n",
    "        - Chọn hành động bằng cách sử dụng hàm get_target_action\n",
    "        - Thực hiện hành động và nhận trạng thái tiếp theo, phần thưởng và trạng thái hoàn thành\n",
    "        - Tính toán tổng phần thưởng\n",
    "        - Nếu tổng phần thưởng lớn hơn hoặc bằng 200, tăng biến thành công\n",
    "        - Trả về số lượng thành công và tỷ lệ thành công\n",
    "        '''\n",
    "        total_success = 0\n",
    "        testing = gym.make(\"LunarLander-v3\")\n",
    "        \n",
    "        for episode in tqdm(range(episodes)):\n",
    "            state, _ = testing.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.get_target_action(state)\n",
    "                state, reward, terminated, truncated, _ = testing.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "            if total_reward >= 200:\n",
    "                total_success += 1\n",
    "        print(f\"Success: {total_success}/{episodes} | Success Rate: {total_success / episodes * 100:.2f}%\")\n",
    "        testing.close()\n",
    "        \n",
    "        \n",
    "    def display_sample_video(self, sample_video=1):\n",
    "        '''\n",
    "        - Hiển thị video mẫu của mô hình đã được huấn luyện\n",
    "        - Sử dụng RecordVideo để ghi lại video trong môi trường\n",
    "        - Trong mỗi tập, khởi tạo trạng thái và phần thưởng\n",
    "        - Lặp qua các bước trong mỗi tập cho đến khi hoàn thành\n",
    "        - Chọn hành động bằng cách sử dụng hàm get_target_action\n",
    "        - Thực hiện hành động và nhận trạng thái tiếp theo, phần thưởng và trạng thái hoàn thành\n",
    "        - Đóng video sau khi hoàn thành\n",
    "        - Trả về video cuối cùng được ghi lại\n",
    "        '''\n",
    "        video_render = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "        video_render = RecordVideo(video_render, \"videos\", episode_trigger=lambda x: True)\n",
    "        \n",
    "        for _ in tqdm(range(sample_video)):\n",
    "            state, _ = video_render.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.get_target_action(state)\n",
    "                state, reward, terminated, truncated, _ = video_render.step(action)\n",
    "                done = terminated or truncated\n",
    "            video_render.close()\n",
    "        video_files = glob.glob(\"videos/*.mp4\")\n",
    "        return Video(video_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cde87932",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn_trainer = DDQNTrain(\n",
    "    env=win_env,\n",
    "    optimizer='adam',\n",
    "    epsilon=1.0,\n",
    "    min_epsilon=0.01,\n",
    "    decay=0.995,\n",
    "    gamma=0.99,\n",
    "    batch_size=128,\n",
    "    episodes=2000,\n",
    "    target_update_freq=10,\n",
    "    memory_size=100000,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d40f40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards, losses, fuel_usage = ddqn_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752319a5",
   "metadata": {},
   "source": [
    "Training DDQN:   0%|          | 0/2000 [00:00<?, ?it/s, Reward=-211, Epsilon=0.995, Loss=0, Fuel Usage=9.48]\n",
    "Episode 0, Average Reward: nan\n",
    "Training DDQN:   5%|▌         | 101/2000 [00:46<16:14,  1.95it/s, Reward=-191, Epsilon=0.603, Loss=2.23e+3, Fuel Usage=6.36]\n",
    "Episode 100, Average Reward: -221.90\n",
    "Training DDQN:  10%|█         | 201/2000 [01:38<14:58,  2.00it/s, Reward=-110, Epsilon=0.365, Loss=1.8e+3, Fuel Usage=11.2]  \n",
    "Episode 200, Average Reward: -138.13\n",
    "Training DDQN:  15%|█▌        | 301/2000 [03:26<47:23,  1.67s/it, Reward=-258, Epsilon=0.221, Loss=1.86e+3, Fuel Usage=11.8]   \n",
    "Episode 300, Average Reward: -131.86\n",
    "Training DDQN:  20%|██        | 401/2000 [05:35<23:06,  1.15it/s, Reward=2.19, Epsilon=0.134, Loss=1.84e+3, Fuel Usage=4.26]  \n",
    "Episode 400, Average Reward: -109.48\n",
    "Training DDQN:  25%|██▌       | 501/2000 [08:25<33:51,  1.36s/it, Reward=-115, Epsilon=0.0812, Loss=2.16e+3, Fuel Usage=10.9]   \n",
    "Episode 500, Average Reward: -106.71\n",
    "Training DDQN:  30%|███       | 601/2000 [11:07<37:20,  1.60s/it, Reward=-141, Epsilon=0.0492, Loss=1.31e+3, Fuel Usage=6.81]  \n",
    "Episode 600, Average Reward: -82.40\n",
    "Training DDQN:  35%|███▌      | 701/2000 [14:19<37:56,  1.75s/it, Reward=-125, Epsilon=0.0298, Loss=1.34e+4, Fuel Usage=76.7]   \n",
    "Episode 700, Average Reward: 18.65\n",
    "Training DDQN:  40%|████      | 801/2000 [16:43<23:58,  1.20s/it, Reward=236, Epsilon=0.018, Loss=3.97e+3, Fuel Usage=19.6]   \n",
    "Episode 800, Average Reward: -81.40\n",
    "Training DDQN:  45%|████▌     | 901/2000 [18:55<30:23,  1.66s/it, Reward=-217, Epsilon=0.0109, Loss=7.54e+3, Fuel Usage=37.8] \n",
    "Episode 900, Average Reward: -52.22\n",
    "Training DDQN:  50%|█████     | 1001/2000 [21:01<16:14,  1.02it/s, Reward=-11.7, Epsilon=0.01, Loss=2.17e+3, Fuel Usage=19.8] \n",
    "Episode 1000, Average Reward: -11.73\n",
    "Training DDQN:  55%|█████▌    | 1101/2000 [23:06<22:50,  1.52s/it, Reward=103, Epsilon=0.01, Loss=7.52e+3, Fuel Usage=38.7]  \n",
    "Episode 1100, Average Reward: 113.59\n",
    "Training DDQN:  60%|██████    | 1201/2000 [25:05<13:59,  1.05s/it, Reward=235, Epsilon=0.01, Loss=4.84e+3, Fuel Usage=23.2]  \n",
    "Episode 1200, Average Reward: 158.15\n",
    "Training DDQN:  65%|██████▌   | 1301/2000 [27:03<13:36,  1.17s/it, Reward=240, Epsilon=0.01, Loss=4.75e+3, Fuel Usage=30.1]  \n",
    "Episode 1300, Average Reward: 192.16\n",
    "Training DDQN:  70%|███████   | 1401/2000 [29:00<09:37,  1.04it/s, Reward=235, Epsilon=0.01, Loss=3.23e+3, Fuel Usage=11.8]  \n",
    "Episode 1400, Average Reward: 216.21\n",
    "Training DDQN:  75%|███████▌  | 1501/2000 [30:56<08:47,  1.06s/it, Reward=229, Epsilon=0.01, Loss=3.44e+3, Fuel Usage=21.5]  \n",
    "Episode 1500, Average Reward: 211.82\n",
    "Training DDQN:  80%|████████  | 1601/2000 [32:56<06:04,  1.09it/s, Reward=205, Epsilon=0.01, Loss=2.14e+3, Fuel Usage=15.1]  \n",
    "Episode 1600, Average Reward: 205.14\n",
    "Training DDQN:  85%|████████▌ | 1701/2000 [34:37<04:03,  1.23it/s, Reward=265, Epsilon=0.01, Loss=2.46e+3, Fuel Usage=19.8]  \n",
    "Episode 1700, Average Reward: 213.45\n",
    "Training DDQN:  90%|█████████ | 1801/2000 [36:08<02:50,  1.17it/s, Reward=267, Epsilon=0.01, Loss=3.65e+3, Fuel Usage=26.2]  \n",
    "Episode 1800, Average Reward: 193.01\n",
    "Training DDQN:  95%|█████████▌| 1901/2000 [37:37<01:26,  1.15it/s, Reward=246, Epsilon=0.01, Loss=5.43e+3, Fuel Usage=21.8]  \n",
    "Episode 1900, Average Reward: 167.89\n",
    "Training DDQN: 100%|██████████| 2000/2000 [38:56<00:00,  1.17s/it, Reward=250, Epsilon=0.01, Loss=4.83e+3, Fuel Usage=16.8]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "089d69c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_trainer.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d57e9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_trainer.save_model(\"ddqn_lunarlander-win-env-2k-adam.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e001ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_trainer.load_model(\"ddqn_lunarlander-win-env-2k-adam.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ee651933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_trainer.test_model(episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d97523",
   "metadata": {},
   "source": [
    "100%|██████████| 500/500 [00:22<00:00, 21.95it/s]\n",
    "Success: 471/500 | Success Rate: 94.20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "64e45110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_trainer.display_sample_video(sample_video=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a306e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos-ddqn-wind/rl-video-episode-4.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"videos-ddqn-wind/rl-video-episode-4.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f009628",
   "metadata": {},
   "source": [
    "Có thể thấy DDQN có tốc độ học nhỉnh hơn so với DQN (38p và 40p) trong khi độ chính xác tăng cao\n",
    "- Đạt AVG-Reward > 200 tại ep1400 trong khi DQN đạt > 200 tại ep1700\n",
    "- Đạt success rate 93% (có thể hiệu suất bị giảm trong 200ep cuối do mô hình chưa ổn định, nếu train lâu hơn có thể đạt độ chính xác cao hơn)\n",
    "\n",
    "Lý do: \n",
    "- Giảm overestimation nhờ tách biệt quá trình chọn và đánh giá hành động.\n",
    "- Hội tụ ổn định hơn trong môi trường nhiễu.\n",
    "- Chính sách học được tổng quát tốt hơn, giúp tăng độ chính xác và hiệu suất."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
